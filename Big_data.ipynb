{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e354d93-75a1-4704-ac8f-6f4a796df11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3ba15-a8aa-455a-b5b2-bd9e041189e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "dataset.cv_split([0.7, 0.2, 0.1])  \n",
    "for i in range(MAX_ITER):\n",
    "    batch = dataset.train.next_batch(BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e3053-3a80-4084-865e-84a83200c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# sample dummy image tensors\n",
    "image_data = torch.randn(1000, 3, 64, 64) \n",
    "labels = torch.randint(0, 10, (1000,))  \n",
    "\n",
    "dataset = TensorDataset(image_data, labels)\n",
    "\n",
    "#Split into batches\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#to view every iterated batch\n",
    "for batch_images, batch_labels in dataloader:\n",
    "    print(f\"Batch shape: {batch_images.shape}, Labels: {batch_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f0f8c-19a0-4b44-beac-7480bb040fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#preprocess - transform as tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                 download=True, transform=transform)\n",
    "#to describe train\n",
    "train_dataset\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae88c14-3543-48c3-a6ac-5b2153f34d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Стратификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef4ac7-5a8c-4ada-bdd4-815a315735a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Стратифицированное разделение по целевой переменной y\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Проверка распределения классов\n",
    "print(\"Обучающая выборка:\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Тестовая выборка:\", pd.Series(y_test).value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4d3f6-3f7d-4593-80d3-78cbeac4a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание составного признака для стратификации\n",
    "df['strat_feature'] = df['class'] + '_' + df['gender']\n",
    "\n",
    "# Разделение с учетом составного признака\n",
    "train_df, test_df = train_test_split(\n",
    "df, test_size=0.2, random_state=42, \n",
    "stratify=df['strat_feature']\n",
    ")\n",
    "\n",
    "# Удаление временного признака\n",
    "train_df = train_df.drop('strat_feature', axis=1)\n",
    "test_df = test_df.drop('strat_feature', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a780cd5-a775-4dfc-81c5-2bdf2e4b9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "Базовая кросс-валидация с cross val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fa514-16d0-4441-9494-e158edea14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(f\"Accuracy scores for each fold: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {scores.std():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a97bee-17bf-4c54-9787-57ef50321b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Кросс-валидация на временных рядах TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9882527-02c8-4a2e-9084-e121ec9105f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=5,          # сколько раз «катнём» валидацию\n",
    "    test_size=90,        # длина теста в днях/часах/минутках\n",
    "    gap=7,               # буфер между train и test, спасает от look-ahead при лагах\n",
    ")\n",
    "\n",
    "for tr, ts in tscv.split(data):\n",
    "    print(data.index[tr][0], '…', data.index[tr][-1], '→', data.index[ts][0], '…', data.index[ts][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b3f23-31b0-4588-bbf6-8db8eb0b1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "leave one out cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afae7aa-4e8c-4401-b3b1-d9c562895449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "\n",
    "print(f\"LOOCV scores: {scores}\")\n",
    "print(f\"Mean accuracy: {scores.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def30ac8-78ee-4d47-85da-c591d65bf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Создаем объект LOOCV\n",
    "loo = LeaveOneOut()\n",
    "scores = []\n",
    "\n",
    "# Итерируем по разбиениям\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Создаем и обучаем модель\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Предсказание и оценка\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"LOOCV accuracy: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7ee9c-aa47-4ce9-9798-cfa681e8c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf937e-8ca2-4605-9be9-8929ac0f0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from collections import defaultdict, Counter\n",
    "from functools import reduce\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "import re\n",
    "\n",
    "class SimpleMapReduce:\n",
    "    \n",
    "    def __init__(self, num_workers=None):\n",
    "        self.num_workers = num_workers or multiprocessing.cpu_count()\n",
    "    \n",
    "    def map(self, mapper: Callable, data: List[Any]) -> List[Any]:\n",
    "        with multiprocessing.Pool(self.num_workers) as pool:\n",
    "            return pool.map(mapper, data)\n",
    "    \n",
    "    def shuffle(self, mapped_results: List[List[Tuple]]) -> Dict[Any, List[Any]]:\n",
    "        shuffled = defaultdict(list)\n",
    "        for result in mapped_results:\n",
    "            for key, value in result:\n",
    "                shuffled[key].append(value)\n",
    "        return shuffled\n",
    "    \n",
    "    def reduce(self, reducer: Callable, shuffled_data: Dict[Any, List[Any]]) -> Dict[Any, Any]:\n",
    "        results = {}\n",
    "        with multiprocessing.Pool(self.num_workers) as pool:\n",
    "            items = list(shuffled_data.items())\n",
    "            reduced = pool.starmap(reducer, items)\n",
    "            for key, value in reduced:\n",
    "                results[key] = value\n",
    "        return results\n",
    "    \n",
    "    def run(self, data: List[Any], mapper: Callable, reducer: Callable) -> Dict[Any, Any]:\n",
    "        # Map phase\n",
    "        print(\"Running Map phase...\")\n",
    "        mapped = self.map(mapper, data)\n",
    "        \n",
    "        # Shuffle phase\n",
    "        print(\"Running Shuffle phase...\")\n",
    "        shuffled = self.shuffle(mapped)\n",
    "        \n",
    "        # Reduce phase\n",
    "        print(\"Running Reduce phase...\")\n",
    "        reduced = self.reduce(reducer, shuffled)\n",
    "        \n",
    "        return reduced\n",
    "\n",
    "# Пример: WordCount\n",
    "def wordcount_mapper(text: str) -> List[Tuple[str, int]]:\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    return [(word, 1) for word in words]\n",
    "\n",
    "def wordcount_reducer(key: str, values: List[int]) -> Tuple[str, int]:\n",
    "    return (key, sum(values))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"Hello world hello python\",\n",
    "        \"Python is great for data science\",\n",
    "        \"World of python programming\",\n",
    "        \"Data science with python and spark\"\n",
    "    ]\n",
    "    \n",
    "    # Запуск MapReduce\n",
    "    mr = SimpleMapReduce(num_workers=2)\n",
    "    result = mr.run(documents, wordcount_mapper, wordcount_reducer)\n",
    "    \n",
    "    print(\"\\nWord Count Results:\")\n",
    "    for word, count in sorted(result.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903e020-0ca1-4f96-9965-2c891b697ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Large Scal Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208ebdc-d94f-4ea9-82d7-dd199db879f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Iterator\n",
    "from collections import defaultdict, Counter\n",
    "import heapq\n",
    "import hashlib\n",
    "\n",
    "class MapReduceAlgorithms:\n",
    "    \n",
    "    @staticmethod\n",
    "    def word_count(documents: List[str]) -> Dict[str, int]:\n",
    "        from concurrent.futures import ProcessPoolExecutor\n",
    "        \n",
    "        def mapper(document: str) -> List[tuple]:\n",
    "            words = document.lower().split()\n",
    "            return [(word, 1) for word in words]\n",
    "        \n",
    "        def reducer(key: str, values: List[int]) -> tuple:\n",
    "            return (key, sum(values))\n",
    "        \n",
    "        # Map phase\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            mapped = list(executor.map(mapper, documents))\n",
    "        \n",
    "        # Shuffle\n",
    "        shuffled = defaultdict(list)\n",
    "        for doc_results in mapped:\n",
    "            for word, count in doc_results:\n",
    "                shuffled[word].append(count)\n",
    "        \n",
    "        # Reduce phase\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            items = list(shuffled.items())\n",
    "            reduced = list(executor.starmap(reducer, items))\n",
    "        \n",
    "        return dict(reduced)\n",
    "    \n",
    "    @staticmethod\n",
    "    def inverted_index(documents: List[str]) -> Dict[str, List[int]]:\n",
    "        inverted_index = defaultdict(set)\n",
    "        \n",
    "        for doc_id, doc in enumerate(documents):\n",
    "            words = set(doc.lower().split())\n",
    "            for word in words:\n",
    "                inverted_index[word].add(doc_id)\n",
    "        \n",
    "        return {word: list(doc_ids) for word, doc_ids in inverted_index.items()}\n",
    "    \n",
    "    @staticmethod\n",
    "    def distributed_sort(data: List[Any], key=None) -> List[Any]:\n",
    "        import tempfile\n",
    "        import os\n",
    "        \n",
    "        # Шаг 1: Разделение на отсортированные чанки\n",
    "        chunk_size = 10000  # элементов в чанке\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(data), chunk_size):\n",
    "            chunk = data[i:i + chunk_size]\n",
    "            chunk.sort(key=key)\n",
    "            \n",
    "            # Сохраняем чанк во временный файл\n",
    "            with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "                for item in chunk:\n",
    "                    f.write(f\"{item}\\n\")\n",
    "                chunks.append(f.name)\n",
    "        \n",
    "        # Шаг 2: K-way merge\n",
    "        def merge_chunks(chunk_files):\n",
    "            # Открываем все файлы\n",
    "            files = [open(f, 'r') for f in chunk_files]\n",
    "            heap = []\n",
    "            \n",
    "            # Инициализация кучи\n",
    "            for i, f in enumerate(files):\n",
    "                line = f.readline()\n",
    "                if line:\n",
    "                    item = line.strip()\n",
    "                    heapq.heappush(heap, (item, i))\n",
    "            \n",
    "            # Слияние\n",
    "            result = []\n",
    "            while heap:\n",
    "                item, file_idx = heapq.heappop(heap)\n",
    "                result.append(item)\n",
    "                \n",
    "                # Читаем следующую строку из того же файла\n",
    "                next_line = files[file_idx].readline()\n",
    "                if next_line:\n",
    "                    next_item = next_line.strip()\n",
    "                    heapq.heappush(heap, (next_item, file_idx))\n",
    "            \n",
    "            # Закрываем файлы\n",
    "            for f in files:\n",
    "                f.close()\n",
    "            return result\n",
    "        \n",
    "        sorted_data = merge_chunks(chunks)\n",
    "        \n",
    "        # Очистка временных файлов\n",
    "        for chunk_file in chunks:\n",
    "            os.unlink(chunk_file)\n",
    "        \n",
    "        return sorted_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def page_rank(graph: Dict[int, List[int]], damping=0.85, iterations=10) -> Dict[int, float]:\n",
    "        import numpy as np\n",
    "        \n",
    "        # Преобразуем граф в матрицу переходов\n",
    "        nodes = list(graph.keys())\n",
    "        n = len(nodes)\n",
    "        node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "        \n",
    "        # Инициализация\n",
    "        pr = np.ones(n) / n\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            new_pr = np.zeros(n)\n",
    "            \n",
    "            for node in nodes:\n",
    "                idx = node_to_idx[node]\n",
    "                outlinks = graph.get(node, [])\n",
    "                \n",
    "                if outlinks:\n",
    "                    # Распределение PageRank по исходящим ссылкам\n",
    "                    share = pr[idx] / len(outlinks)\n",
    "                    for outlink in outlinks:\n",
    "                        if outlink in node_to_idx:\n",
    "                            outlink_idx = node_to_idx[outlink]\n",
    "                            new_pr[outlink_idx] += share\n",
    "                else:\n",
    "                    # Dead ends - распределяем равномерно\n",
    "                    new_pr += pr[idx] / n\n",
    "            \n",
    "            # Формула PageRank\n",
    "            new_pr = damping * new_pr + (1 - damping) / n\n",
    "            pr = new_pr\n",
    "        \n",
    "        return {node: pr[node_to_idx[node]] for node in nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9603cf-9db1-4823-9a78-24d0e5d72cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c20c17-419a-45be-a213-a3f46724eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "class SGD:\n",
    "    \n",
    "    def __init__(self, lr: float = 0.01, momentum: float = 0.0):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def update(self, params: List[np.ndarray], \n",
    "               grads: List[np.ndarray]) -> None:\n",
    "        if self.velocity is None:\n",
    "            self.velocity = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            if self.momentum > 0:\n",
    "                self.velocity[i] = (self.momentum * self.velocity[i] - \n",
    "                                   self.lr * grads[i])\n",
    "                params[i] += self.velocity[i]\n",
    "            else:\n",
    "                params[i] -= self.lr * grads[i]\n",
    "    \n",
    "    def step(self):\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcc551-2cbe-4abd-bd93-7af25250d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    def __init__(self, lr: float = 0.01, eps: float = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.cache = None\n",
    "    \n",
    "    def update(self, params: List[np.ndarray], grads: List[np.ndarray]) -> None:\n",
    "        if self.cache is None:\n",
    "            self.cache = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            # Накопление квадратов градиентов\n",
    "            self.cache[i] += grads[i] ** 2\n",
    "            \n",
    "            # Обновление параметров\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.cache[i]) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db3c92-2c91-44ba-ba2c-95129614ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    def __init__(self, lr: float = 0.001, alpha: float = 0.99, \n",
    "                 eps: float = 1e-8):\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha  # коэффициент затухания\n",
    "        self.eps = eps\n",
    "        self.cache = None\n",
    "    \n",
    "    def update(self, params: List[np.ndarray], grads: List[np.ndarray]) -> None:\n",
    "        if self.cache is None:\n",
    "            self.cache = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        for i in range(len(params)):\n",
    "            # Экспоненциальное скользящее среднее квадратов градиентов\n",
    "            self.cache[i] = (self.alpha * self.cache[i] + \n",
    "                            (1 - self.alpha) * grads[i] ** 2)\n",
    "            \n",
    "            # Обновление параметров\n",
    "            params[i] -= (self.lr * grads[i] / \n",
    "                         (np.sqrt(self.cache[i]) + self.eps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
